{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this lab you’ll dig a bit deeper into the Scikit-Learn API, to see an example of unsupervised learning. This will complement the discussion of supervised learning in the chapter, where we showed how to use Naïve Bayes classifiers to detect and predict classifications for a dataset.\n",
    "\n",
    "# Roadmap\n",
    "There are 4 exercises in this lab, of which the last exercise is \"if time permits\". Here is a brief summary of the tasks you will perform in each exercise; more detailed instructions follow later:\n",
    "1.\tGetting started with sample data\n",
    "2.\tPerforming principal component analysis on the data\n",
    "3.\tPlotting the components as vectors on top of the datapoints\n",
    "4.\t(If time permits) Reducing the dimensionality of the data\n",
    "\n",
    "# Introduction to dimensionality reduction\n",
    "Machine learning is often applied to very large and complex datasets, containing millions of samples and maybe hundreds of features per sample. Faced with this quantity and richness of data, it can be difficult to detect relationships between features and to understand which features are more important than others.\n",
    "\n",
    "In this kind of scenario, you will typically use dimensionality reduction to reduce the number of dimensions in your feature space. This can make it easier to spot underlying relationships between features, easier to visualize the data in a graph, and faster to process.\n",
    "\n",
    "There are two ways to do dimensionality reduction:\n",
    "-\tFeature elimination – Eliminate features that you think are superfluous or less important. For example, if you have 200 measured features, you might eliminate 190 of them, so that you can focus on just 10 of them. The advantage of feature elimination is that it simplifies your dataset, but the disadvantage is that you lose all information about the features you’ve eliminated.\n",
    "-\tFeature extraction – This is a better approach. It uses all the existing features to create a new set of independent features, ranked according to their influence on the dataset. For example, imagine you have 200 features; you can extract these features into up to 200 independent new features, where each new feature is a combination of all of the 200 “old” features. \n",
    "\n",
    "The cool thing about feature extraction is that you can decide to keep as many of the new independent features as you want, but drop the least important ones (remember, with feature extraction, the features are ranked in order of their influence on the dataset, i.e. in how much a new feature contributes to the dataset values). For example, you might decide to keep the 10 most important new features, and drop the 190 least important ones. The critical thing is that all the new features are combinations of the old features, so you’re still keeping the most valuable parts of your old variables. \n",
    "\n",
    "A common technique for performing feature extraction is principal component analysis (PCA). PCA is a fast and flexible unsupervised learning algorithm and is well supported in Scikit-Learn. In a nutshell, it takes existing data and identifies components (i.e. axes) along which the data shows greatest variance. These components are all orthogonal to each other, i.e. independent of each other. \n",
    "\n",
    "The components are ranked in order of importance/influence. The principal component is the line along which the data has greatest variance, i.e. this component exhibits the greatest influence on the dataset values; the next highest component exhibits the next highest variance, so it has less influence on the dataset values; and so on. \n",
    "\n",
    "You decide how many components you want to retain – you can pick just the top few, and you’ll know these have the greatest influence on the data. For example, you could take a 200-feature dataset and use PCA to reduce it to the 10 principal components – these 10 principal components are like an x-y-z graph in 10-dimensionsal space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1:  Getting started with sample data\n",
    "\n",
    "In this exercise you’ll start working with some sample data, which will serve as a simple example of how to use PCA to reduce the dimensionality of a dataset. It will also help you understand how “components” work in PCA.\n",
    "\n",
    "The following cell is the starting point for this lab, it contains code that\n",
    "\n",
    "-\tCreates a 100-by-2 array of random numbers, representing 100 (x, y) points. In machine learning terminology, this is a dataset where n_samples=100 and n_features=2.\n",
    "-\tPrints the entire array, i.e. 100 (x, y) values.\n",
    "-\tSplices off column 0 in the array (i.e. the x values), and prints it.\n",
    "-\tSplices off column 1 in the array (i.e. the y values) and prints it.\n",
    "-\tDraws the 100 (x, y) points on a scatterplot graph.\n",
    "\n",
    "Execute the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set aesthetic parameters for Seaborn in one convenient step, to make the graphs look nice.\n",
    "sns.set()\n",
    "\n",
    "# Create a NumPy RandomState object, which has convenience methods for generating random numbers and matrices.\n",
    "# For full details, see https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.RandomState.html.\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# Generate 100 random (x,y) points as follows:\n",
    "#   rng.rand(2, 2)    - Creates a 2x2 random matrix with values between 0 and 1.\n",
    "#   rng.randn(2, 100) - Creates a random 2x100 matrix.\n",
    "#   dot()             - Performs a dot product on the two matrices, creates a 2x100 result matrix.\n",
    "#   transpose()       - Transposes the 2x100 matrix into a 100x2 matrix, i.e. 100 (x,y) points.\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 100)).transpose()\n",
    "print(\"All 100 (x,y) points\\n\", X)\n",
    "print(\"\\nColumn 0 (x values)\\n\", X[:, 0])  \n",
    "print(\"\\nColumn 1 (y values)\\n\", X[:, 1])  \n",
    "\n",
    "# Draw a scatterplot graph showing the 100 (x,y) points.\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.4)\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2:  Performing principal component analysis on the data\n",
    "\n",
    "\n",
    "In this exercise you’ll use a principal component analysis (PCA) object from Scikit-Learn to model the relationship between the (x, y) features in the sample dataset. \n",
    "\n",
    "You’ll configure the PCA object to identify 2 principal components for the data. These principal components will be like orthogonal vectors that show the 2 main axes along which the variance of values is greatest (for a better understanding of where this is heading, take a look at the graph on the next page, which shows these two components plotted as vectors on top of the datapoints).\n",
    "So, let’s get started. The first thing to do is to import the PCA class from Scikit-Learn as follows:\n",
    "\n",
    "    from sklearn.decomposition import PCA \n",
    "\n",
    "Next, create a Scikit-Learn PCA model object to find 2 principal components:\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "n_components can be any value up to n_features for the dataset. Here, n_components and n_features are both 2, so you haven’t actually reduced the dimensionality (you’ll see how to do that later in the lab). You may be wondering whether there’s any point in using PCA if you don’t reduce the dimensionality – the answer is “yes”, because PCA calculates the axes along which the data shows greatest variance, and this can help you understand the data better.\n",
    "\n",
    "Now fit the PCA model object to the data, i.e. compute the 2 principal components for the data:\n",
    "\n",
    "    pca.fit(X)\n",
    "\n",
    "After you've fitted the PCA model object to the data, the PCA object has two useful attributes:\n",
    "-\tcomponents_\n",
    "\n",
    "Array of shape (n_components, n_features), indicating how much each feature influences each principal component. The bigger the number, the bigger the influence that a feature has on a component. \n",
    "-\texplained_variance_\n",
    "\n",
    "Array of shape (n_components), indicating the amount of variance explained by each component. The first principal component explains the highest variance, the second principal component explains the next highest variance, and so on.    \n",
    "\n",
    "Print the value of these two attributes as follows:\n",
    "\n",
    "    print(\"PCA components_\\n\", pca.components_)\n",
    "    print(\"PCA explained_variance_\\n\", pca.explained_variance_)\n",
    "\n",
    "The absolute numbers don’t matter, it’s the relative values that matter (e.g. how much more significant is one feature than another on a given component, and what’s the relative importance of each component on the data). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACE YOUR SOLUTION HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3:  Plotting the components as vectors on top of the datapoints\n",
    "\n",
    "In this exercise you’ll draw another scatterplot graph, with the 2 principal components drawn as vectors on top of the datapoints - see the diagram below. Note the white vectors – these represent the two principal components, and indicate the lines along which the data variance is greatest. The longer the vector, the greater the variance of data along this axis.\n",
    "\n",
    "To help you out, we’ve provided a helper file named helpers.py. It has a function named draw_vector(), which draws a vector from point v0 to point v1 on a graph. To be able to make use of this function, add the following statement in reduceSimpleData.py:\n",
    "    from helpers import draw_vector  \n",
    "\n",
    "Now add the following code in reduceSimpleData.py, to draw a scatterplot graph showing the original datapoints again:\n",
    "   \n",
    "   plt.scatter(X[:, 0], X[:, 1], alpha=0.4)\n",
    "\n",
    "Now add the following code, to draw the 2 principal components as vectors on top of the datapoints (copy-and-paste this into your code, and see below for explanations):\n",
    "\n",
    "    for len, vec in zip(pca.explained_variance_, pca.components_):\n",
    "        v = vec * 3 * np.sqrt(len)\n",
    "        draw_vector(pca.mean_, pca.mean_ + v)\n",
    "\n",
    "-\tpca.explained_variance_ is an array of shape (n_components,). Each row contains a single value that indicates the variance (i.e. importance) of a component.\n",
    "-\tpca.components_ is an array of shape (n_components, n_features). Each row specifies the axis for a component, expressed in terms of the importance of the (x, y) features on that component. For example, if a row in pca.components_ contains (5, 1) then the x feature is 5 times more important than the y feature for this component. We’ll therefore draw this component as a vector where the x length is 5 times longer than the y length. \n",
    "-\tzip() zips these two arrays together into a collection of tuples that looks like this:\n",
    ">    [\n",
    ">        ( comp0_expVar,  [comp0_feat0, comp0_feat1] ),\n",
    ">        ( comp1_expVar,  [comp1_feat0, comp1_feat1] )\n",
    ">    ]\n",
    "-\tWe loop through this collection of tuples, and assign an explained_variance_ to len and a component_ to vec.\n",
    "-\tInside the loop, we do a bit of math to scale the vector by its length (components with a larger variance will be drawn longer). Then we call draw_vector() to draw the vector, starting at pca.mean (the computed mean point for the data). \n",
    "-\tYou can now show the graph (containing the datapoints and component vectors) as follows:\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACE YOUR SOLUTION HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4:  Reducing the dimensionality of the data\n",
    "\n",
    "Up until now, you’ve been using a PCA model object that computed 2 principal components for 2D data (x, y values). \n",
    "\n",
    "Now, you’ll create a PCA model object with just 1 principal component – effectively the long vector in the previous graph. This is an example of dimensionality reduction, i.e. you will reduce 2D data (x, y values) into 1D data values along the principal component axis. \n",
    "\n",
    "By the end of the exercise, you’ll have drawn the following graph (the dark points are the projections of the data points along the principal component axis, i.e. having removed the influence of the less important principal component axis):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACE YOUR SOLUTION HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
