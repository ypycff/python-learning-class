{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" width=\"120\" src=\"../Images/supplier-logo.png\">\n",
    "<img style=\"float: left; margin-top: 0\" width=\"80\" src=\"../Images/client-logo.png\">\n",
    "<br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis\n",
    "\n",
    "This notebook will explain the following topics and concepts:\n",
    "\n",
    "**Ranking Data** \n",
    "- Ascending and Descending\n",
    "\n",
    "**Concatenation**\n",
    "- Rows and Columns\n",
    "\n",
    "**Merging Data**\n",
    "- left\n",
    "- right\n",
    "- inner\n",
    "- outer\n",
    "\n",
    "**Joining Data**\n",
    "- left\n",
    "- right\n",
    "- inner\n",
    "- outer\n",
    "\n",
    "**Grouping Data**\n",
    "- by time\n",
    "- by columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries & Load Data\n",
    "\n",
    "We'll use the same csv files as we used in chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# format for floats\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GOOGL = pd.read_excel('../Data/market_data.xls', sheet_name='GOOGL', index_col='Date', parse_dates=True)\n",
    "df_IBM = pd.read_excel('../Data/market_data.xls', sheet_name='IBM', index_col='Date', parse_dates=True)\n",
    "df_MSFT = pd.read_excel('../Data/market_data.xls', sheet_name='MSFT', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Data\n",
    "\n",
    "- We can compute a numerical rank from one through to the number of valid data points.\n",
    "- This can be applied to Series and DataFrames.\n",
    "- Use the **rank()** method.\n",
    "- Often used in conjunction with sort.\n",
    "- In the case of a tie, the default behaviour is to assign the mean to each tied group.\n",
    "\n",
    "Let's try a simple example to clarify the concept of ranking\n",
    "  - We'll add a column called rank to the IBM DataFrame, based on the AdjVolume.<br>\n",
    "  - This will create a new column called 'rank'.\n",
    "  - The 'rank' column will contain a number from 0 to n for each row, indicating the rank of that row's AdjVolume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IBM['rank'] = df_IBM['Volume'].rank()\n",
    "display(df_IBM.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we sort the table according to the 'rank' column then it should also be sorted according to 'Volume'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_IBM.sort_values(by=\"rank\").head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a more realistic example:\n",
    "\n",
    "Rank the difference in AdjVol between Google and IBM, for all days in 2017 where AdjVol for Google > AdjVol for IBM\n",
    "\n",
    "There are a few ways to achieve this, the simplest is to create a new DataFrame to store all the adjusted Volumes for IBM and Google for the year 2017 and then use this to filter and rank.\n",
    "\n",
    "## Note that \n",
    "for most days in 2017, the Volume for IBM > Volume for Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IBM['Volume']['2017'].plot()\n",
    "df_GOOGL['Volume']['2017'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank the days where Volume for Google > Volumne for IBM< for the year 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few ways to achieve this\n",
    "# Easiest to understand is create a new DataFrame for holding Volumes\n",
    "df_Vol = pd.DataFrame()\n",
    "\n",
    "# Add the AdjVolume for 2017 of IBM and Google as separate columns in our new DataFrame\n",
    "df_Vol['IBM'] = df_IBM['2017']['Volume']\n",
    "df_Vol['Google'] = df_GOOGL['2017']['Volume']\n",
    "\n",
    "# Create a new column containing the difference\n",
    "df_Vol['Diff'] = df_Vol['Google'] - df_Vol['IBM']\n",
    "\n",
    "# Now use a filter (same as in lesson 3) to find where Google is greater than IBM\n",
    "goog_gt_ibm = df_Vol['Google'] > df_Vol['IBM']\n",
    "\n",
    "# Get the rows in Google's AdjVolume that match the filter and rank them\n",
    "# We haven't sorted them so they're still displayed in order of the index (Date)\n",
    "df_Vol[['Diff']][goog_gt_ibm].rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the results of the filter in a temporary DataFrame\n",
    "# Note the use of copy - create a copy of the data rather than a reference to it\n",
    "df_tmp = df_Vol[goog_gt_ibm].copy()\n",
    "\n",
    "# Calculate the differences and store these values as a new column\n",
    "df_tmp['Diff)'] = df_tmp['Google'] - df_tmp['IBM']\n",
    "\n",
    "# Add a new ranking column\n",
    "df_tmp['Rank'] = df_tmp['Diff)'].rank()\n",
    "\n",
    "# Sort\n",
    "df_tmp.sort_values(by='Rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also rank in the other direction (highest number gets a rank of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank top to Bottom and Sort\n",
    "df_tmp['Rank'] = df_tmp['Diff'].rank(ascending=False)\n",
    "df_tmp.sort_values(by='Rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation\n",
    "\n",
    "- Glues together DataFrames, without much intelligence.\n",
    "- Dimensions should match along the axis you are concatenating on. \n",
    "- Use **pd.concat** and pass in a list of DataFrames to concatenate together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a few simple DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = pd.date_range(start='2017', freq='BQ', periods=4)\n",
    "cols  = ['Open', 'Close']\n",
    "\n",
    "df1 = df_IBM.reindex(date_range)[cols]\n",
    "df2 = df_GOOGL.reindex(date_range)[cols]\n",
    "df3 = df_MSFT.reindex(date_range)[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate\n",
    "\n",
    "The default is to concatenate the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df1, df2, df3])\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate the columns\n",
    "Pass the `axis = 1` parameter to pd.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df1, df2, df3], axis=1)\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging\n",
    "\n",
    "Pandas has two important functions for joining DataFrames together which intelligently try to align values from selected columns of each DataFrame. These functions are called **merge** and **join**. These functions use a similar logic to joins in SQL.\n",
    "\n",
    "First we will look at merge.\n",
    "\n",
    "**There are 4 Different types of merge**\n",
    "- **Inner Merge** – The default Pandas behaviour, only keep rows where the merge “on” value exists in both the left and right DataFrames.\n",
    "- **Left Merge** – (aka left merge or left join) Keep every row in the left DataFrame. Where there are missing values of the “on” variable in the right DataFrame, add empty / NaN values in the result.\n",
    "- **Right Merge** – (aka right merge or right join) Keep every row in the right DataFrame. Where there are missing values of the “on” variable in the left column, add empty / NaN values in the result.\n",
    "- **Outer Merge** – A full outer join returns all the rows from the left DataFrame, all the rows from the right DataFrame, and matches up rows where possible, with NaNs elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some sample DataFrames\n",
    "\n",
    "Just a few days worth of Data from Google and IBM\n",
    "\n",
    "Note the difference in date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['High', 'Low']\n",
    "\n",
    "df1 = df_IBM[cols]['2017-Jan-01':'2017-Jan-06'].sort_index()\n",
    "df2 = df_GOOGL[cols]['2017-Jan-05':'2017-Jan-10'].sort_index()\n",
    "\n",
    "# show both dataframes\n",
    "print(\"== IBM ==\")\n",
    "display(df1)\n",
    "print(\"== GOOGLE ==\")\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Merge\n",
    "\n",
    "Only keep values for Dates found in both left (df1) and right (df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of merge works in latest Pandas\n",
    "pd.merge(df1, df2, how='inner', on='Date')\n",
    "\n",
    "# This version of merge works in older Pandas\n",
    "pd.merge(df1, df2, how='inner', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left Merge\n",
    "\n",
    "- Keep everything in the left DataFrame.\n",
    "- Where nothing exists in the right DataFrame, fill with NaN (\"Not a Number\" - these are empty values).\n",
    "- Use the suffixes parameter to override the x_ and y_ defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs = ['_IBM','_GOOGL']\n",
    "\n",
    "pd.merge(df1, df2, how='left', on='Date', suffixes=srcs)\n",
    "\n",
    "pd.merge(df1, df2, how='left', left_index=True, right_index=True,  suffixes=srcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right Merge\n",
    "\n",
    "- Keep everything in the right DataFrame\n",
    "- Where nothing exists in the left DataFrame, fill with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='right', on='Date', suffixes=srcs)\n",
    "\n",
    "pd.merge(df1, df2, how='right', left_index=True, right_index=True, suffixes=srcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer Merge\n",
    "\n",
    "Keep everything in both left and right DataFrames, fill with NaN where no data present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='outer', on='Date', suffixes=srcs)\n",
    "\n",
    "pd.merge(df1, df2, how='outer', on='Date', left_index=True, right_index=True, suffixes=srcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining\n",
    "\n",
    "- The second pandas function for intelligently combining DataFrames is called **join**.\n",
    "- Join is **very** similar to merge.\n",
    "- As with merge, the **how** parameter takes inner, outer, left or right.\n",
    "- As with merge, the **on** parameter is the name of a column to join on.\n",
    "- However there is one major difference:\n",
    "  - When using join the \"on\" **must** be the index in at least one of the DataFrames.\n",
    "  - Merge will allow the \"on\" to be a regular column in **both** DataFrames.\n",
    "\n",
    "The syntax for calling the two functions is also slightly different:\n",
    "- **join**  : df1.join(df2, how=\"inner\", on=\"Date\")\n",
    "- **merge** : pd.merge(df1, df2, how=\"inner\", on=\"Date\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = ['High', 'Low']\n",
    "cols2 = ['Open', 'Close']\n",
    "df1 = df_IBM[cols1]['2017-Jan-01':'2017-Jan-07'].sort_index()\n",
    "df2 = df_IBM[cols2]['2017-Jan-04':'2017-Jan-11'].sort_index()\n",
    "\n",
    "# show both dataframes\n",
    "print(\"== High & Low ==\")\n",
    "display(df1)\n",
    "print(\"== Open & Close ==\")\n",
    "display(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we don't specify an \"on\" then we will join \"on\" the index of both DataFrames\n",
    "df1.join(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, how='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, how='outer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Data\n",
    "\n",
    "Pandas provides functions that allow us to group rows of data together and call aggregate functions on them as a unit e.g. mean, max, min, std, etc.\n",
    "\n",
    "To create a group we call the **groupby** method on a DataFrame.\n",
    "\n",
    "e.g. Create groups where the 'Industry' column is the same:\n",
    "    - df1.groupby('Industry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Columns\n",
    "\n",
    "Use the **by** parameter and supply a column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(io='../Data/sample_data.xls', sheet_name='Groups', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping is a convenient way to get the mean for each sector of each column\n",
    "df.groupby(by='Sector').mean()\n",
    "\n",
    "# Grouping is a convenient way to get the mean for each sector of each column\n",
    "df.groupby(by='Rep').mean()\n",
    "\n",
    "# Grouping is a convenient way to get the mean for each sector of each column\n",
    "df.groupby(by='Portfolio').mean()\n",
    "\n",
    "# Or we could create a variable to store the name of the function\n",
    "func = 'std'\n",
    "df.groupby(by='Sector').agg(func)\n",
    "\n",
    "# Or we can create a list of functions to aggregate with\n",
    "funcs = ['median', 'std']\n",
    "df.groupby(by='Rep').agg(funcs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Time Period\n",
    "Often we want to group according to a frequency e.g. a group of all values in a single business quarter.\n",
    "\n",
    "We can then call mean for all of the groups, e.g. get the mean Volume per business quarter.\n",
    "\n",
    "A convenient way to group by a frequency is to use the Grouper object (in the pandas namespace).\n",
    "\n",
    "- pd.Grouper\n",
    "- Most commonly used to pass in a frequency\n",
    "- Group by frequencies: B (business day), BQ (business quarter), M (month), Y (year), etc.\n",
    "- It's also possible to group by specific time frequencies e.g. 15d, 1h30min etc.\n",
    "  - See the list of frequency aliases: http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(pd.Grouper(freq='BQ')).mean()\n",
    "\n",
    "# Or we could create a variable to store the grouper\n",
    "by_BMonth = pd.Grouper(freq='BM')\n",
    "df.groupby(by=by_BMonth).mean()\n",
    "\n",
    "# Or use teh agg and a list of functions\n",
    "funcs = ['median', 'std']\n",
    "df.groupby(by=by_BMonth).agg(funcs)\n",
    "\n",
    "\n",
    "# Finally we can supply a list of groupers\n",
    "by_BMonth = pd.Grouper(freq='BM')\n",
    "by_Rep = pd.Grouper(key='Rep')\n",
    "by_Sector = pd.Grouper(key='Sector')\n",
    "\n",
    "# And perform a variety of slice and dice operations\n",
    "groups1 = [by_Rep, by_BMonth, by_Sector]\n",
    "df.groupby(by=groups1).agg(funcs)\n",
    "\n",
    "groups2 = [by_Sector, by_Rep, by_BMonth]\n",
    "df.groupby(by=groups2).agg(funcs)\n",
    "\n",
    "groups3 = [by_BMonth, by_Sector, by_Rep]\n",
    "df.groupby(by=groups3).agg(funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GOOGL['Adj. Open'].groupby(pd.Grouper(freq='Q')).mean()\n",
    "df_GOOGL['Adj. Open'].groupby(pd.Grouper(freq='Q')).mean().plot()\n",
    "\n",
    "df_GOOGL['2017']['Adj. Open'].groupby(pd.Grouper(freq='d')).mean()\n",
    "df_GOOGL['2017']['Adj. Open'].rolling(30).mean()\n",
    "df_GOOGL['2017']['Adj. Open'].rolling(30, min_periods=3).mean()\n",
    "\n",
    "df_GOOGL['2017']['Adj. Open'].groupby(pd.Grouper(freq='15d')).mean()\n",
    "df_GOOGL['Adj. Open'].groupby(pd.Grouper(freq='BQ')).mean()\n",
    "\n",
    "df_GOOGL['Adj. Open'].groupby(pd.Grouper(freq='BQ')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = pd.date_range(start='2019', freq='BM', periods=12)\n",
    "df = pd.read_excel(io='../Data/sample_data.xls', sheet_name='Groups')\n",
    "del df['Price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame()\n",
    "\n",
    "for i in rng:\n",
    "    tmp = df.copy()\n",
    "    tmp.insert(loc=0, column='Date', value=i)\n",
    "    his = np.random.randint(98, 103, tmp['High'].shape[0]) / 100\n",
    "    tmp['High'] = tmp['High'] * his\n",
    "    tmp['Low'] = tmp['Low'] * 0.98\n",
    "    df_new = pd.concat([df_new, tmp])\n",
    "    \n",
    "pd.concat([df_new.head(), df_new.tail()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.index = df_new['Date']\n",
    "del df_new['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(98, 103, 5) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Grouper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w = pd.ExcelWriter(path='../Data/sample_data.xls')\n",
    "\n",
    "#df_new.to_excel(excel_writer=w, sheet_name='NewGroups')\n",
    "\n",
    "#w.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
